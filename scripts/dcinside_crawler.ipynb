{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'data' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-831a01d3f20d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[0mpos_g6\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m4460000\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[0mpos_g6\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m4500000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m     \u001b[0mcrawler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_g6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'g6'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m     \u001b[0mpos_g6\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-831a01d3f20d>\u001b[0m in \u001b[0;36mget_page\u001b[1;34m(self, pos, types)\u001b[0m\n\u001b[0;32m     48\u001b[0m                 \u001b[0mlink\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlink_text\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'icon_notice'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#운영자의 공지는 크롤링 제외\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrawl_everything\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-831a01d3f20d>\u001b[0m in \u001b[0;36mcrawl_everything\u001b[1;34m(self, link)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'e'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html5lib'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'data' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import datetime\n",
    "import re\n",
    "import codecs\n",
    "import threading\n",
    "\n",
    "def find_a(tags):\n",
    "    return tags.name == 'a' and tags.has_attr('href') and tags.has_attr('class')\n",
    "\n",
    "def find_table(tags):\n",
    "    return tags.name == 'table' and not tags.has_attr('id') and tags.has_attr('class')\n",
    "\n",
    "def find_a_not_class(tags):\n",
    "    return tags.name == 'a' and not tags.has_attr('target')\n",
    "\n",
    "class DCinsidePostsCrawler(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_page(self, pos, types):\n",
    "        url = 'http://gall.dcinside.com/board/lists/?id=smartphone&page={}&search_pos={}&s_type=search_all&s_keyword={}'\n",
    "        no = 1\n",
    "        while True:\n",
    "            url = url.format(no, pos, types)\n",
    "            hdr = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',\n",
    "                   'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'}\n",
    "            try:\n",
    "                data = requests.get(url, headers=hdr)\n",
    "            except requests.exceptions.ConnectionError:\n",
    "                return\n",
    "            content = data.content\n",
    "            soup = BeautifulSoup(content, 'html5lib')\n",
    "            table = soup.find('tbody', attrs={'class': 'list_tbody'})\n",
    "\n",
    "            group = table.find_all(find_a)\n",
    "\n",
    "            if group[-1].find('td',attrs={'class':'t_hits'}) == '': # 페이지 범위 넘어가면 공지만 존재\n",
    "                break\n",
    "\n",
    "            for a in group:\n",
    "                link_text = a['class']\n",
    "                link = a['href']\n",
    "                if link_text != 'icon_notice': #운영자의 공지는 크롤링 제외\n",
    "                    self.crawl_everything(link)\n",
    "                else:\n",
    "                    pass\n",
    "            no += 1\n",
    "\n",
    "\n",
    "    def crawl_everything(self, link):\n",
    "        url = 'https://gall.dcinside.com' + link\n",
    "        hdr = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',\n",
    "               'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'}\n",
    "        try:\n",
    "            data = requests.get(url, headers=hdr)\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print('e')\n",
    "\n",
    "        content = data.content\n",
    "\n",
    "        soup = BeautifulSoup(content, 'html5lib')\n",
    "\n",
    "        for script in soup(['script','style']):\n",
    "            script.extract()\n",
    "\n",
    "        title = soup.find('dd')\n",
    "        if title == None or title == '':\n",
    "            return\n",
    "        title = title.text\n",
    "        title = re.sub(',','',title)\n",
    "\n",
    "\n",
    "        contents = soup.find('div', attrs={'class':'s_write'})\n",
    "        if contents == None or contents == '':\n",
    "            return\n",
    "        content = contents.find('td')\n",
    "        content = \" \".join(content.text.split())\n",
    "        content = re.sub(',','', content)\n",
    "\n",
    "        w_top_right = soup.find('div', attrs={'class':'w_top_right'})\n",
    "        date = w_top_right.find('b')\n",
    "        if date == None or date == ':':\n",
    "            return\n",
    "        date = date.text\n",
    "\n",
    "        w_top_left = soup.find('div', attrs={'class':'w_top_left'})\n",
    "        read = w_top_left.find_all('dd', attrs={'class':'dd_num'})[0]\n",
    "        read = read.text.strip()\n",
    "\n",
    "        try:\n",
    "            #self.boarddao.save_posts(str(link), str(date), str(title), str(content), str(read))\n",
    "\n",
    "            with codecs.open('dcinside_g6.csv','a','utf-8') as fa:\n",
    "                print(date)\n",
    "                writer = csv.writer(fa)\n",
    "                writer.writerow([str(link), str(date), title, str(content), str(read), datetime.datetime.now()])\n",
    "\n",
    "        except Exception, e:\n",
    "            print(e)\n",
    "\n",
    "crawler = DCinsidePostsCrawler()\n",
    "pos_g6 = -4460000\n",
    "while pos_g6 >= -4500000:\n",
    "    crawler.get_page(pos_g6,'g6')\n",
    "    pos_g6 -= 10000"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py2]",
   "language": "python",
   "name": "conda-env-py2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
